Imputed values
value_counts
no     36548
yes     4640
total  41118

%
no  89%
yes 11%

######### 1st RUN #########
Data is scaled
Test size 30%, random state = 89
imputation=mean

--------- KNN ---------
n_neighhbors = 100

             precision    recall  f1-score   support

         no       0.93      0.97      0.95     10967
        yes       0.64      0.39      0.49      1390

avg / total       0.89      0.91      0.90     12357

[[10662   305]
 [  844   546]]
 
accuracy = 0.91

--------- Decision Tree ---------
max_depth = 5

             precision    recall  f1-score   support

         no       0.94      0.95      0.95     10967
        yes       0.60      0.54      0.57      1390

avg / total       0.90      0.91      0.91     12357

[[10471   496]
 [  644   746]]
 
accuracy = 0.91

--------- Random Forest ---------
n_estimators=200

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.62      0.48      0.54      1390

avg / total       0.90      0.91      0.90     12357

[[10547   420]
 [  718   672]]
 
accuracy = 0.91
 
--------- Naive Bayes ---------

             precision    recall  f1-score   support

         no       0.95      0.92      0.93     10967
        yes       0.49      0.61      0.54      1390

avg / total       0.90      0.88      0.89     12357

[[10081   886]
 [  546   844]]
 
accuracy = 0.88

######### 2nd RUN #########
Data is scaled
Test size 30%, random state = 89
imputation=mean

--------- KNN ---------
n_neighhbors = 10

             precision    recall  f1-score   support

         no       0.93      0.97      0.95     10967
        yes       0.62      0.41      0.50      1390

avg / total       0.89      0.91      0.90     12357

[[10617   350]
 [  815   575]]

accuracy = 0.91

--------- Decision Tree ---------
max_depth = 10

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.59      0.51      0.55      1390

avg / total       0.90      0.91      0.90     12357

[[10475   492]
 [  675   715]]

accuracy = 0.91

--------- Random Forest ---------
n_estimators=10

             precision    recall  f1-score   support

         no       0.93      0.96      0.95     10967
        yes       0.60      0.43      0.50      1390

avg / total       0.89      0.90      0.90     12357

[[10569   398]
 [  792   598]]
 
accuracy = 0.90 

######### 3rd RUN #########
Data is scaled
Test size 30%, random state = 89
imputation=mean

--------- KNN ---------
n_neighhbors = 500

             precision    recall  f1-score   support

         no       0.92      0.97      0.95     10967
        yes       0.65      0.36      0.47      1390

avg / total       0.89      0.91      0.89     12357

[[10690   277]
 [  883   507]]
 
accuracy = 0.91

--------- Decision Tree ---------
max_depth = 100

             precision    recall  f1-score   support

         no       0.94      0.94      0.94     10967
        yes       0.51      0.50      0.50      1390

avg / total       0.89      0.89      0.89     12357

[[10291   676]
 [  700   690]]
 
accuracy = 0.90

--------- Random Forest ---------
n_estimators=1000

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.61      0.49      0.55      1390

avg / total       0.90      0.91      0.90     12357

[[10540   427]
 [  708   682]]
 
accuracy = 0.91

######### 4th RUN #########
Data is not scaled
Test size 30%, random state = 89
imputation=mean

--------- KNN ---------
n_neighhbors = 100

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.63      0.48      0.55      1390

avg / total       0.90      0.91      0.90     12357

[[10576   391]
 [  721   669]]
 
accuracy = 0.91

--------- Decision Tree ---------
max_depth = 5

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.63      0.51      0.56      1390

avg / total       0.90      0.91      0.91     12357

[[10543   424]
 [  679   711]]
 
accuracy = 0.91

--------- Random Forest ---------
n_estimators=200

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.65      0.53      0.58      1390

avg / total       0.91      0.92      0.91     12357

[[10621   346]
 [  727   663]]

accuracy = 0.91

--------- Naive Bayes ---------

             precision    recall  f1-score   support

         no       0.94      0.91      0.92     10967
        yes       0.42      0.52      0.47      1390

avg / total       0.88      0.87      0.87     12357

[[10574   393]
 [  655   735]]
 
accuracy = 0.92

######### 5th RUN #########
Data is scaled
Test size 30%, random state = 89
imputation=most_frequent

--------- KNN ---------
n_neighhbors = 100

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.63      0.48      0.55      1390

avg / total       0.90      0.91      0.90     12357

[[10576   391]
 [  721   669]]
 
accuracy = 0.91

--------- Decision Tree ---------
max_depth = 5

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.63      0.51      0.56      1390

avg / total       0.90      0.91      0.91     12357

[[10543   424]
 [  679   711]]
 
accuracy = 0.91

--------- Random Forest ---------
n_estimators=200

             precision    recall  f1-score   support

         no       0.94      0.96      0.95     10967
        yes       0.65      0.53      0.59      1390

avg / total       0.91      0.92      0.91     12357

[[10621   346]
 [  727   663]]
 
accuracy = 0.91

--------- Naive Bayes ---------

             precision    recall  f1-score   support

         no       0.94      0.87      0.90     10967
        yes       0.36      0.59      0.45      1390

avg / total       0.88      0.84      0.85     12357

[[9503 1464]
 [ 563  827]]
 
accuracy = 0.84

 
Deleted values
value_counts
no     26629
yes     3859
total  30488

%
no  87%
yes 13%

######### 6th RUN #########
Data is scaled
Test size 30%, random state = 89
imputation=deletion

--------- KNN ---------
n_neighhbors = 100

             precision    recall  f1-score   support

         no       0.92      0.97      0.95      8024
        yes       0.66      0.40      0.50      1123

avg / total       0.89      0.90      0.89      9147

[[7796  228]
 [ 671  452]]
 
accuracy = 0.90

--------- Decision Tree ---------
max_depth = 5

             precision    recall  f1-score   support

         no       0.95      0.94      0.95      8024
        yes       0.61      0.65      0.63      1123

avg / total       0.91      0.91      0.91      9147

[[7554  470]
 [ 396  727]]
 
accuracy = 0.90
 
--------- Random Forest ---------
n_estimators=200

             precision    recall  f1-score   support

         no       0.94      0.96      0.95      8024
        yes       0.62      0.54      0.58      1123

avg / total       0.90      0.90      0.90      9147

[[10621   346]
 [  727   663]]
 
accuracy = 0.91
 
--------- Naive Bayes ---------

             precision    recall  f1-score   support

         no       0.94      0.92      0.93      8024
        yes       0.50      0.57      0.53      1123

avg / total       0.88      0.88      0.88      9147

[[7383  641]
 [ 486  637]]
 
accuracy = 0.88
